{"cells":[{"cell_type":"code","source":["df1 = spark.sql(\"\"\" select * from TestJsonFinal \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read data from Delta Table","showTitle":true,"inputWidgets":{},"nuid":"a79de81a-bef8-4236-aa5b-29162823b8a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----------------------------+\n|Difficulty|Avg_Total_Cooking_Time_inMin|\n+----------+----------------------------+\n|      EASY|                        30.0|\n|    MEDIUM|                        55.0|\n|      HARD|                       230.0|\n+----------+----------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----------------------------+\n|Difficulty|Avg_Total_Cooking_Time_inMin|\n+----------+----------------------------+\n|      EASY|                        30.0|\n|    MEDIUM|                        55.0|\n|      HARD|                       230.0|\n+----------+----------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df1 = spark.sql(\"\"\" select \ncase when (cookTime_in_minutes+prepTime_in_minutes) <= 30 then \"EASY\"\n     when (cookTime_in_minutes+prepTime_in_minutes) > 30 and (cookTime_in_minutes+prepTime_in_minutes) <= 60 then \"MEDIUM\" \n     when (cookTime_in_minutes+prepTime_in_minutes) > 60 then \"HARD\" END as Difficulty,\navg(cookTime_in_minutes+prepTime_in_minutes) as Avg_Total_Cooking_Time_inMin   from TestJsonFinal where  ingredients like '%Beef%' or ingredients like '%beef%' \ngroup by Difficulty \"\"\")\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Business Logic","showTitle":true,"inputWidgets":{},"nuid":"be4dd595-fb6a-4409-a35a-a60d4ea959ef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#import pandas as pd\n#dbutils.fs.ls(\"dbfs:/FileStore/tables/json\")\ndf1.write.format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .saveAsTable(\"Output\")\n#df1.write.option(\"header\",True).csv(\"/FileStore/tables/json/outfinal.csv\")\n#df1.write.format(\"csv\").save(\"/FileStore/tables/json/out.csv\")\n#dbutils.fs.rm(\"/FileStore/tables/json/out.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Load Final Output As CSV","showTitle":true,"inputWidgets":{},"nuid":"2c0b9b66-fabb-48e8-950c-4913a903e2fe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#df2 = spark.read.csv(\"dbfs:/FileStore/tables/json/outfinal.csv\")\ndf2 = spark.sql(\"\"\"select * from Output\"\"\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Show Final Output","showTitle":true,"inputWidgets":{},"nuid":"c5c644d2-342a-46bb-83c6-048d3277dd89"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_Task2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3477262169968947}},"nbformat":4,"nbformat_minor":0}
