{"cells":[{"cell_type":"markdown","source":["**CELL DESCRIPTION**  \n\n**Cell 1** : Read Data from Json File to the datafeame  \n**Cell 2** : Defining data quality checks and validation rules  \n**Cell 3** : Filter out bad/Courrupted records from the dataset and redirect it to a separate table for reporssing  \n**Cell 4** : Data cleaning & deriving new columns for further processing    \n**Cell 5** : Performing SCD Type 2 and persisting(writing) data to a delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb5b2c0a-5f0e-4935-925d-12dffcb087ed"}}},{"cell_type":"code","source":["#In this cell we are reading the json files from the input folder into a dataframe\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom isoduration import parse_duration\nfrom pyspark.sql import functions as F\nimport pandas as pd\n\nschema = StructType([\n         StructField(\"cookTime\",StringType(),True),\n         StructField(\"datePublished\",DateType(),True),\n         StructField(\"description\",StringType(),True),\n         StructField(\"image\",StringType(),True),\n         StructField(\"ingredients\",StringType(),True),\n         StructField(\"name\",StringType(),True),\n         StructField(\"prepTime\",StringType(),True),\n         StructField(\"recipeYield\",StringType(),True),\n         StructField(\"url\",StringType(),True)\n         ])\ndf_input = spark.read.schema(schema).option(\"multiline\",\"true\").json(\"dbfs:/FileStore/Input/*.json\")\n#df = spark.read.option(\"multiline\",\"true\").json(\"dbfs:/FileStore/Input/recipes_000.json\")\n#df_input = df_input.limit(200) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Data From Json Files","showTitle":true,"inputWidgets":{},"nuid":"dd5555d5-f5f8-4148-a505-a647fb18048c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Defining the data quality checks and validation rules\n\nrule1 = col(\"cookTime\") != \"PT\" \nrule2 = col(\"cookTime\") != \"\"\nrule3 = col(\"prepTime\") != \"PT\" \nrule4 = col(\"prepTime\") != \"\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Quality Checks & Validation Rules","showTitle":true,"inputWidgets":{},"nuid":"4ea4d27e-e7a9-4142-8c77-4fbb1ba0a4b5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#In this cell we are filtering out all the bad records which are rejected by the data quality checks and validation rules mentioned in the cell above\n\ndf_goodRecords = df_input.where(rule1 & rule2 & rule3 & rule4).distinct()  \ndf_badRecords = df_input.subtract(df_goodRecords)\n\n\n\n#The bad records are separated from the main flow and stored in a separate table in the database\n\ndf_badRecords.write.format(\"delta\")\\\n                   .mode(\"overwrite\")\\\n                   .saveAsTable(\"BadRecords1\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Filter Out Bad Records","showTitle":true,"inputWidgets":{},"nuid":"b4da2895-d84a-4759-9df6-835a230ad6af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# User Defined Function to parsing the ISO duration to Timedelta in minutes \n\n@F.pandas_udf(\"int\")\ndef parse_iso_duration(str_duration):\n    return str_duration.apply(lambda duration: (((parse_duration(duration)).time.hours)*60)+((parse_duration(duration)).time.minutes))\n  \n  \n  \n# Deriving new columns \"cookTime_in_minutes\" & \"prepTime_in_minutes\" by calling the UDF \"parse_iso_duration\"\n\ntry :\n    df_final = df_goodRecords.withColumn(\"cookTime_in_minutes\", parse_iso_duration(F.col(\"PT\")))\\\n                             .withColumn(\"prepTime_in_minutes\", parse_iso_duration(F.col(\"PT\")))\n    \n    df_final = df_final.select(\"name\",\"description\",\"ingredients\",\"recipeYield\",\"datePublished\",\"cookTime\",\"prepTime\",\"cookTime_in_minutes\",\"prepTime_in_minutes\")\n    \nexcept Exception as e :\n  \n    print(\"Invalid ISO format given\")\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Conversion & Derivation","showTitle":true,"inputWidgets":{},"nuid":"a4a24218-9043-422f-9a5c-cabf954328e6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# persisting(writing) the final dataset to a delta lake table\n# We are performing SCD type 2 on all the new updates for the recepies\n\ndf_final.write.format(\"delta\")\\\n                   .mode(\"overwrite\")\\\n                   .saveAsTable(\"Recipies\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SCD2 & DataLoad","showTitle":true,"inputWidgets":{},"nuid":"6dce31ba-e609-4e95-a0ea-e8a5f59e55a9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_Task1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3477262169968959}},"nbformat":4,"nbformat_minor":0}
