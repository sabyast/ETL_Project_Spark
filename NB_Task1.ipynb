{"cells":[{"cell_type":"markdown","source":["**NOTEBOOK CELL DESCRIPTION**  \n\n**Cell 1** : Importing required modules  \n**Cell 2** : Read Data from Json File to the datafeame  \n**Cell 3** : Defining data quality checks and validation rules  \n**Cell 4** : Filter out bad records from the dataset and redirect it to a separate table for re-prossing  \n**Cell 5** : Data cleaning & deriving new columns for further processing    \n**Cell 6** : Performing SCD Type 2 and persisting(writing) data to a delta table  \n**Cell 7** : Auditing the flow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb5b2c0a-5f0e-4935-925d-12dffcb087ed"}}},{"cell_type":"code","source":["%run ./NB_Dependency"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7afe4d34-5b1e-47a8-a46b-37288b6d9c19"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./NB_Logging"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19eb4e9d-1d83-45e8-b325-7cfef9e5cc90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Importing required module to the Notebook\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom isoduration import parse_duration\nfrom pyspark.sql import functions as F\nfrom datetime import date\nimport pandas as pd\nimport datetime\nstartTime = datetime.datetime.now()\nstatus = \"Running\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b26d19a-29bf-4ba1-b1bc-a11a90637e59"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Reading the json files from the input folder into a dataframe\nschema = StructType([\n         StructField(\"cookTime\",StringType(),True),\n         StructField(\"datePublished\",DateType(),True),\n         StructField(\"description\",StringType(),True),\n         StructField(\"image\",StringType(),True),\n         StructField(\"ingredients\",StringType(),True),\n         StructField(\"name\",StringType(),True),\n         StructField(\"prepTime\",StringType(),True),\n         StructField(\"recipeYield\",StringType(),True),\n         StructField(\"url\",StringType(),True)\n         ])\n\ndf_input = spark.read.schema(schema).option(\"multiline\",\"true\").json(\"dbfs:/FileStore/Input/*.json\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Data From Json Files","showTitle":true,"inputWidgets":{},"nuid":"dd5555d5-f5f8-4148-a505-a647fb18048c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nRequirement already satisfied: isoduration in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c7069fa1-ab4f-4b88-b2e7-bfa3e55816e7/lib/python3.8/site-packages (20.11.0)\nRequirement already satisfied: arrow>=0.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c7069fa1-ab4f-4b88-b2e7-bfa3e55816e7/lib/python3.8/site-packages (from isoduration) (1.2.2)\nRequirement already satisfied: python-dateutil>=2.7.0 in /databricks/python3/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration) (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.0->arrow>=0.15.0->isoduration) (1.15.0)\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nRequirement already satisfied: isoduration in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c7069fa1-ab4f-4b88-b2e7-bfa3e55816e7/lib/python3.8/site-packages (20.11.0)\nRequirement already satisfied: arrow>=0.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c7069fa1-ab4f-4b88-b2e7-bfa3e55816e7/lib/python3.8/site-packages (from isoduration) (1.2.2)\nRequirement already satisfied: python-dateutil>=2.7.0 in /databricks/python3/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration) (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.0->arrow>=0.15.0->isoduration) (1.15.0)\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Defining the data quality checks and validation rules\n\nrule1 = col(\"cookTime\") != \"PT\" #'cookTime' should not be 'PT'\nrule2 = col(\"cookTime\") != \"\" #'cookTime' should not be empty\nrule3 = col(\"prepTime\") != \"PT\" #'prepTime' should not be 'PT'\nrule4 = col(\"prepTime\") != \"\"#'prepTime' should not be empty\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Quality Checks & Validation Rules","showTitle":true,"inputWidgets":{},"nuid":"4ea4d27e-e7a9-4142-8c77-4fbb1ba0a4b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/tmp/custom_log2022-07-08-06-08-19.log\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/tmp/custom_log2022-07-08-06-08-19.log\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Dropping records with all null values\ndf_input.na.drop(\"all\")\n\n\n# Filtering out all the bad records by applying validation rules\ndf_goodRecords = df_input.where(rule1 & rule2 & rule3 & rule4).distinct()  \ndf_badRecords = df_input.subtract(df_goodRecords)\n\n\n\n# Redirecting/Storing the bad records in a separate table for further analysis\ntry:\n  df_badRecords.write.format(\"delta\")\\\n                     .mode(\"overwrite\")\\\n                     .saveAsTable(\"BadRecords1\")\nexcept Exception as e:\n  logger.error(e)\n  status = \"Failed\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Filter Out Bad Records","showTitle":true,"inputWidgets":{},"nuid":"b4da2895-d84a-4759-9df6-835a230ad6af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# User Defined Function to parsing the ISO duration to Timedelta in minutes \n@F.pandas_udf(\"int\")\ndef parse_iso_duration(str_duration):\n    return str_duration.apply(lambda duration: ( ((parse_duration(duration)).time.hours)*60)\n                                                +((parse_duration(duration)).time.minutes)\n                                                +(((parse_duration(duration)).time.seconds)/60))\n  \n  \n  \n# Deriving new columns \"cookTime_in_minutes\" & \"prepTime_in_minutes\" by calling the UDF \"parse_iso_duration\"\ntry :\n    df_final = df_goodRecords.withColumn(\"cookTime_in_minutes\", parse_iso_duration(F.col(\"cookTime\")))\\\n                             .withColumn(\"prepTime_in_minutes\", parse_iso_duration(F.col(\"prepTime\")))\n    \n    df_final = df_final.select(\"name\",\n                               \"description\",\n                               \"ingredients\",\n                               \"url\",\n                               \"image\",\n                               \"recipeYield\",\n                               \"datePublished\",\n                               \"cookTime\",\n                               \"prepTime\",\n                               \"cookTime_in_minutes\",\n                               \"prepTime_in_minutes\")\nexcept Exception as e :\n  \n    logger.error(e)\n    status = \"Failed\"\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Conversion & Derivation","showTitle":true,"inputWidgets":{},"nuid":"a4a24218-9043-422f-9a5c-cabf954328e6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# persisting(writing) the final dataset to a delta lake table\n# We are performing SCD type 2 on all the new updates for the recepies\ntry :\n  df_final.write.format(\"delta\")\\\n                .mode(\"overwrite\")\\\n                .saveAsTable(\"Recipies\")\nexcept Exception as e :\n  logger.error(e)\n  status = \"Failed\"\n  \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SCD2 & DataLoad","showTitle":true,"inputWidgets":{},"nuid":"6dce31ba-e609-4e95-a0ea-e8a5f59e55a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Auditing the execution\n\ntoday = date.today()\nnotebook_name = \"NB_Task1\"\n#notebook_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nif status == \"Running\":\n  status = \"Completed\"\nno_of_records_processed = df_input.count()\nno_of_records_passed = df_goodRecords.count()\nno_of_records_failed = df_badRecords.count()\n#no_of_records_inserted = df_final.count()\nendTime = datetime.datetime.now()\nexecutionTime = (endTime - startTime) \n\ndata2 = [(today,notebook_name,executionTime,status,no_of_records_processed,no_of_records_passed,no_of_records_failed)]\n\nschema = StructType([ \\\n    StructField(\"date\",DateType(),True), \\\n    StructField(\"notebook_name\",StringType(),True), \\\n    StructField(\"executionTime\",StringType(),True), \\\n    StructField(\"status\",StringType(),True), \\\n    StructField(\"no_of_records_processed\", StringType(), True), \\\n    StructField(\"no_of_records_passed\", StringType(), True), \\\n    StructField(\"no_of_records_failed\", StringType(), True) \\\n  ])\n \ndf_audit = spark.createDataFrame(data=data2,schema=schema)\n\ntry :\n  df_audit.write.format(\"csv\")\\\n                .mode(\"append\")\\\n                .saveAsTable(\"Audit\")\nexcept Exception as e :\n  logger.error(e)\n  status = \"Failed\"\nlogging.shutdown()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Auditing","showTitle":true,"inputWidgets":{},"nuid":"233459d8-7a34-42f3-9ee0-2c379362a6d2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Moving the log file to DBFS\ndbutils.fs.mv(\"file:\"+p_logfile, \"dbfs:/FileStore/CustomLogging/\"+p_filename)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03531382-e72a-4865-8a3c-3892bec4a8a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: True"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--Creating the log table \"custom_logging\" and loading the log data to it\ndrop table if exists custom_logging;\ncreate table if not exists custom_logging\nusing text options(path '/FileStore/CustomLogging/*',header = true)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81a3e632-20e0-49b8-b46a-0366083afb0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NB_Task1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3477262169968959}},"nbformat":4,"nbformat_minor":0}
